# ChillChill Capsule — Living Blueprint
_Last updated: 

## Architecture
- UI: Next.js (Docker, port 3000)
- API: FastAPI /chat, /health
- Vector: Qdrant · Cache: Redis

## Providers & Personas
- LLM_PROVIDER: **ollama**
- LLM_MODEL: **llama3.2:3b**
- Autoswitch order: ****
- Personas: GP→Gemini; Chef→OpenAI; Accountant→Groq (llama3-70b-8192)

## Runtime Config
- CHAT_ECHO: ****
- docker-compose: C:\AiProject\docker-compose.yml
- docker-compose.override: C:\AiProject\docker-compose.override.yml

## Effective docker compose (excerpt)
~~~yaml
name: aiproject services:   api:     build:       context: C:\AiProject\chatbot\agent-api       dockerfile: Dockerfile     container_name: chill_api     depends_on:       redis:         condition: service_healthy         required: true       vector:         condition: service_started         required: true     environment:       ALLOWED_ORIGIN: http://localhost:3000,http://127.0.0.1:3000       API_KEY: dev-15515372eb6f       API_PORT: "8000"       API_URL: http://api:8000/chat       AUTOSWITCH_ENABLED: "true"       AUTOSWITCH_ORDER: gemini,groq,ollama,openai       CORS_ALLOW_ORIGINS: http://localhost:3000       EMBED_MODEL_OPENAI: text-embedding-3-small       GEMINI_API_KEY: ""       GEMINI_ENABLED: "1"       GEMINI_MODEL: gemini-1.5-flash       GOOGLE_API_KEY: ""       GROQ_API_KEY: ""       GROQ_ENABLED: "1"       GROQ_MODEL: llama3-70b-8192       LLM_MODEL: llama3.2:3b       LLM_PROVIDER: ollama       LOG_LEVEL: DEBUG       MISTRAL_API_KEY: ""       NEXT_PUBLIC_API_BASE_URL: http://127.0.0.1:8000       NO_PROXY: localhost,127.0.0.1,api,ui,chill_api,chill_ui,host.docker.internal       OLLAMA_EMBED_MODEL: nomic-embed-text       OLLAMA_ENABLED: "1"       OLLAMA_HOST: http://host.docker.internal:11434       OLLAMA_HOST_DOCKER: http://host.docker.internal:11434       OLLAMA_HOST_HOST: http://127.0.0.1:11434       OLLAMA_MODEL: llama3.2:3b       OPENAI_API_KEY: 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       OPENAI_ENABLED: "false"       PIPER_HOST: piper       PIPER_PORT: "10200"       PROVIDER_DEBUG: "1"       PROVIDER_INIT_TIMEOUT_MS: "5000"       PROVIDER_ORDER: gemini,groq,ollama,openai       PROVIDER_REQUEST_TIMEOUT_MS: "25000"       PYTHONUNBUFFERED: "1"       RAG_COLLECTION: chill_docs       RATE_LIMIT_PER_MINUTE: "60"       REDIS_HOST: redis       REDIS_PORT: "6379"       SOFT_FALLBACK_ON_FAILURE: "true"       VECTOR_HOST: vector       VECTOR_PORT: "6333"       VOICE_ENABLED: "true"       VOICE_LANG: en-AU       VOICE_PROFILE: coqui-cheerful-female       VOICE_PROVIDER: coqui       VOICE_SAMPLE_RATE: "24000"     extra_hosts:       - host.docker.internal=host-gateway     healthcheck:       test:         - CMD         - wget         - -qO-         - http://localhost:8000/health       timeout: 3s       interval: 10s       retries: 6       start_period: 15s     networks:       default: null     ports:       - mode: ingress         target: 8000         published: "8000"         protocol: tcp     restart: unless-stopped   redis:     container_name: chill_redis     healthcheck:       test:         - CMD         - redis-cli         - ping       timeout: 2s       interval: 5s       retries: 30       start_period: 5s     image: redis:alpine     networks:       default: null     ports:       - mode: ingress         target: 6379         published: "6379"         protocol: tcp     restart: unless-stopped   ui:     build:       context: C:\AiProject\chatbot\chatbot-ui       dockerfile: Dockerfile     container_name: chill_ui     depends_on:       api:         condition: service_healthy         required: true     environment:       ALLOWED_ORIGIN: http://localhost:3000,http://127.0.0.1:3000       API_KEY: dev-15515372eb6f       API_URL: http://api:8000/chat       AUTOSWITCH_ENABLED: "true"       AUTOSWITCH_ORDER: gemini,groq,ollama,openai       CORS_ALLOW_ORIGINS: http://localhost:3000       EMBED_MODEL_OPENAI: text-embedding-3-small       GEMINI_API_KEY: ""       GEMINI_ENABLED: "1"       GEMINI_MODEL: gemini-1.5-flash       GOOGLE_API_KEY: ""       GROQ_API_KEY: ""       GROQ_ENABLED: "1"       GROQ_MODEL: llama3-70b-8192       LLM_MODEL: llama3.2:3b       LLM_PROVIDER: ollama       LOG_LEVEL: DEBUG       MISTRAL_API_KEY: ""       NEXT_PUBLIC_API_BASE_URL: http://127.0.0.1:8000       NO_PROXY: localhost,127.0.0.1,api,ui,chill_api,chill_ui,host.docker.internal       NODE_ENV: production       OLLAMA_EMBED_MODEL: nomic-embed-text       OLLAMA_ENABLED: "1"       OLLAMA_HOST: http://host.docker.internal:11434       OLLAMA_HOST_DOCKER: http://host.docker.internal:11434       OLLAMA_HOST_HOST: http://127.0.0.1:11434       OLLAMA_MODEL: llama3.2:3b       OPENAI_API_KEY: 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       OPENAI_ENABLED: "false"       PIPER_HOST: piper       PIPER_PORT: "10200"       PROVIDER_DEBUG: "1"       PROVIDER_INIT_TIMEOUT_MS: "5000"       PROVIDER_ORDER: gemini,groq,ollama,openai       PROVIDER_REQUEST_TIMEOUT_MS: "25000"       RAG_COLLECTION: chill_docs       RATE_LIMIT_PER_MINUTE: "60"       SOFT_FALLBACK_ON_FAILURE: "true"       VECTOR_HOST: vector       VECTOR_PORT: "6333"       VOICE_ENABLED: "true"       VOICE_LANG: en-AU       VOICE_PROFILE: coqui-cheerful-female       VOICE_PROVIDER: coqui       VOICE_SAMPLE_RATE: "24000"     healthcheck:       test:         - CMD         - wget         - -qO-         - http://localhost:3000/       timeout: 3s       interval: 7s       retries: 30       start_period: 15s     networks:       default: null     ports:       - mode: ingress         target: 3000         published: "3000"         protocol: tcp     restart: unless-stopped   vector:     container_name: chill_vector     image: qdrant/qdrant:latest     networks:       default: null     ports:       - mode: ingress         target: 6333         published: "6333"         protocol: tcp     restart: unless-stopped     volumes:       - type: volume         source: vector_data         target: /qdrant/storage         volume: {} networks:   default:     name: aiproject_default volumes:   vector_data:     name: aiproject_vector_data 
~~~
